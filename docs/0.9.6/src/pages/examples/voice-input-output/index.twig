{% extends "../../../layout.twig" %}

{% set page_title = 'Voice control (2/2)' %}
{% set page_slug = '/examples/' %}


{% block page %}
    <h1><a href="#reactjs" id="reactjs" class="anchor"></a>{{page_title}}</h1>
    
    <div class="row cfdoc-example-row">
    	<div class="col-md-12 col-lg-6">
            <p><strong>THE VOICE INPUT IS BROKEN IN THIS EXAMPLE. WE'RE ON IT!</strong></p>
            <p>Fill out a form using your voice. This example uses HTML5s <a href="https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesisUtterance" target="_blank">SpeechSynthesisUtterance</a> and <a href="https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition" target="_blank">SpeechRecognition</a> APIs to have <u>text 2 speech</u> and <u>speech 2 text</u>.</p>
    <p>Read WIKI <a href="../../functionality/voice/" target="_blank">here</a>.</p>
    <p>See a simpler example <a href="../voice-input/" target="_self">here</a>.</p>
    		
            <p>The form:</p>
            <!-- Conversational Form will auto-run because of attribute "cf-form" -->
            <form id="form">
                <input
                    id="123"
                    name="123"
                    type="text"
                    cf-questions="Hello, please tell me your name?"
                />

                <fieldset cf-questions="Choose your favourite color, <span style='background: blue;'>blue</span>, <span style='background: red;'>red</span> or <span style='background: yellow;'>yellow</span>">
                    <input type="radio" cf-label="blue" value="blue" id="1" />
                    <input type="radio" cf-label="red" value="red" id="2" />
                    <input type="radio" cf-label="yellow" value="yellow" id="3" />
                </fieldset>
            </form>

    	</div>
    	<div class="col-md-12 col-lg-6 cfdoc-example">
    		<div id="cf-context" role="cf-context" cf-context></div>
    	</div>

        <script>
            function initExample(){

                var dispatcher = new cf.EventDispatcher(),
                    synth = null,
                    recognition = null,
                    msg = null,
                    SpeechSynthesisUtterance = null,
                    SpeechRecognition = null;

                try{
                    SpeechRecognition = SpeechRecognition || webkitSpeechRecognition;
                }catch(e){
                    console.log("Example support range: https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition#Browser_compatibility");
                }

                try{
                    SpeechSynthesisUtterance = window.webkitSpeechSynthesisUtterance ||
                    window.mozSpeechSynthesisUtterance ||
                    window.msSpeechSynthesisUtterance ||
                    window.oSpeechSynthesisUtterance ||
                    window.SpeechSynthesisUtterance;
                }catch(e){
                    console.log("Example support range: https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesisUtterance#Browser_compatibility")
                }

                // here we use https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API
                // you can use what ever API you want, ex.: Google Cloud Speech API -> https://cloud.google.com/speech/

                // here we create our input
                if(SpeechSynthesisUtterance && SpeechRecognition){
                    var microphoneInput = {
                        init: function() {
                            // init is called one time, when the custom input is instantiated.

                            // load voices \o/
                            synth = window.speechSynthesis;
                            msg = new SpeechSynthesisUtterance();
                            window.speechSynthesis.onvoiceschanged = function(e) {
                                var voices = synth.getVoices();
                                msg.voice = voices[0]; // <-- Alex
                                msg.lang = msg.voice.lang; // change language here
                            };
                            synth.getVoices();

                            // here we want to control the Voice input availability, so we don't end up with speech overlapping voice-input
                            msg.onstart = function(event) {
                                // on message end, so deactivate input
                                console.log("voice: deactivate 1")
                                conversationalForm.userInput.deactivate();
                            }

                            msg.onend = function(event) {
                                // on message end, so reactivate input
                                conversationalForm.userInput.reactivate();
                            }

                            // setup events to speak robot response
                            dispatcher.addEventListener(cf.ChatListEvents.CHATLIST_UPDATED, function(event){
                                if(event.detail.currentResponse.isRobotResponse){
                                    // https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesisUtterance
                                    // msg.text = event.detail.currentResponse.response
                                    msg.text = event.detail.currentResponse.strippedSesponse//<-- no html tags
                                    window.speechSynthesis.speak(msg);
                                }
                            }, false);

                            // do other init stuff, like connect with external APIs ...
                        },
                        // set awaiting callback, as we will await the speak in this example
                        awaitingCallback: true,
                        cancelInput: function() {
                            console.log("voice: CANCEL")
                            finalTranscript = null;
                            if(recognition){
                                recognition.onend = null;
                                recognition.onerror = null;
                                recognition.stop();
                            }
                        },
                        input: function(resolve, reject, mediaStream) {
                            console.log("voice: INPUT")
                            // input is called when user is interacting with the CF input button (UserVoiceInput)

                            // connect to Speech API (ex. Google Cloud Speech), Watson (https://github.com/watson-developer-cloud/speech-javascript-sdk) or use Web Speech API (like below), resolve with the text returned..
                            // using Promise pattern -> https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/Promise
                                // if API fails use reject(result.toString())
                                // if API succedes use resolve(result.toString())
                            
                            if(recognition)
                                recognition.stop();

                            recognition = new SpeechRecognition(),
                                finalTranscript = '';

                            recognition.continuous = false; // react only on single input
                            recognition.interimResults = false; // we don't care about interim, only final.
                            
                            // recognition.onstart = function() {}
                            recognition.onresult = function(event) {
                                // var interimTranscript = "";
                                for (var i = event.resultIndex; i < event.results.length; ++i) {
                                    if (event.results[i].isFinal) {
                                        finalTranscript += event.results[i][0].transcript;
                                    }
                                }
                            }

                            recognition.onerror = function(event) {
                                reject(event.error);
                            }

                            recognition.onend = function(event) {
                                if(finalTranscript && finalTranscript !== ""){
                                    resolve(finalTranscript);
                                }
                            }

                            recognition.start();
                        }
                    }
                }

                var conversationalForm = window.cf.ConversationalForm.startTheConversation({
                    formEl: document.getElementById("form"),
                    context: document.getElementById("cf-context"),
                    eventDispatcher: dispatcher,

                    // add the custom input (microphone)
                    microphoneInput: microphoneInput,

                    submitCallback: function(){
                        // remove Conversational Form
                        console.log("voice: Form submitted...", conversationalForm.getFormData(true));
                        alert("You made it! Check console for data")
                    }
                });

                if(!SpeechRecognition){
                    conversationalForm.addRobotChatResponse("SpeechRecognition not supported, so <strong>no</strong> Microphone here.");
                }

                if(!SpeechSynthesisUtterance){
                    conversationalForm.addRobotChatResponse("SpeechSynthesisUtterance not supported, so <strong>no</strong> Microphone here.");
                }
            }
        </script>

        <style>
            .custom-template{
                font-size:12px;
                color:red;
            }

            main.content section[role=form] form > input{
                margin-bottom: 10px;
            }
            main.content section[role=form] form > label{
                display: block;
            }
            main.content section[role=form] form > fieldset > label, main.content section[role=form] form > fieldset > input{
                display: inline-block !important;
                width: auto;
            }
        </style>

    </div>
	
{% endblock %}